{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MassGH2023/Deep-Learning-and-Neural-Network/blob/main/regression_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1sIk0OlJiJt"
      },
      "source": [
        "<center>\n",
        "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"300\" alt=\"cognitiveclass.ai logo\">\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4uuonQqJiJw"
      },
      "source": [
        "# **Performing Linear Regression in Keras**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCk3CFgCJiJx"
      },
      "source": [
        "Estimated time needed: **30** minutes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2lVPqLcJiJx"
      },
      "source": [
        "During the pandemic, a bike-sharing service had suffered considerable losses in their revenues. As a data scientist on the analytics team, you have been tasked to help with coming up with a business plan to accelerate the company's revenue. The first step in this process is to understand the current demand for shared bikes among the people in the city.\n",
        "\n",
        "You recommend that the team first try to understand the factors on which the demand for these shared bikes depends. In this lab, we will perform a linear regression to predict the count of total rental bikes, and understand the impact of some contributing factors.\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://live.staticflickr.com/36/84666910_b889a5ad04_b.jpg\" width=\"300\" alt=\"tulip flower\">\n",
        "<center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vf-UE56JiJx"
      },
      "source": [
        "## __Table of Contents__\n",
        "\n",
        "<ol>\n",
        "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
        "    <li>\n",
        "        <a href=\"#Setup\">Setup</a>\n",
        "        <ol>\n",
        "            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n",
        "            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n",
        "            <li><a href=\"#Defining-Helper-Functions\">Defining Helper Functions</a></li>\n",
        "        </ol>\n",
        "    </li>\n",
        "    <li>\n",
        "        <a href=\"#Linear regression\">Linear regression</a>\n",
        "        <ol>\n",
        "            <li><a href=\"#Bike-sharing-dataset\">Bike sharing dataset</a></li>\n",
        "            <li><a href=\"#Clean-the-data\">Clean the data</a></li>\n",
        "            <li><a href=\"#Data-exploration-:-Bivariate-analysis\">Data exploration: Bivariate analysis</a></li>\n",
        "            <li><a href=\"#Data exploration-:-Multivariate-analysis\">Data exploration: Multivariate analysis</a></li>\n",
        "            <li><a href=\"#Data exploration-:-Heatmap-of-columns\">Data exploration: Heatmap of columns</a></li>\n",
        "            <li><a href=\"#Split-the-data-into-training-and-test sets\">Split the data into training and test sets</a></li>\n",
        "            <li><a href=\"#Split-features-from-labels\">Split features from labels</a></li>\n",
        "            <li><a href=\"#Normalization\">Normalization</a></li>\n",
        "            <li><a href=\"#Linear-regression-with-a-single-input\">Linear regression with a single input</a></li>\n",
        "            <li><a href=\"#Linear-regression-with-a-multiple-inputs\">Linear regression with a multiple inputs</a></li>\n",
        "        </ol>\n",
        "    </li>\n",
        "</ol>\n",
        "\n",
        "<a href=\"#Exercises\">Exercises</a>\n",
        "<ol>\n",
        "    <li><a href=\"#Exercise-1---Loading-a-dataset-(Auto-MPG)\">Exercise 1. Loading a dataset (Auto MPG)</a></li>\n",
        "    <li><a href=\"#Exercise-2---Clean-the-data\">Exercise 2. Clean the data</a></li>\n",
        "    <li><a href=\"#Exercise-3---Split-the-data\">Exercise 3. Split the data</a></li>\n",
        "    <li><a href=\"#Exercise-4---Create-a-normalization-layer\">Exercise 4. Create a normalization layer</a></li>\n",
        "    <li><a href=\"#Exercise-5---Linear-regression-with-multiple-inputs\">Exercise 5. Linear regression with multiple inputs</a></li>\n",
        "</ol>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PM9VYNtJiJy"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "After completing this lab you will be able to:\n",
        "\n",
        " - Build single-variable and multi-variable linear regression models using Keras.\n",
        " - Use linear regression to predict a continuous target variable given a set of input features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UCqK6KdJiJy"
      },
      "source": [
        "----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNZqdQJtJiJz"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFpCG-NeJiJ0"
      },
      "source": [
        "For this lab, we will be using the following libraries:\n",
        "\n",
        "*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n",
        "*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n",
        "*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n",
        "*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for visualizing the data.\n",
        "*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aaYaaKkJiJ2"
      },
      "source": [
        "### Installing Required Libraries\n",
        "\n",
        "The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run these notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!pip` in the following code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEL8vAz4JiJ2"
      },
      "outputs": [],
      "source": [
        "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n",
        "# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n",
        "# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX6gR5eJJiJ3"
      },
      "source": [
        "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-rHRCtiJiJ3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --upgrade tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax2PNs6VJiJ3"
      },
      "source": [
        "### Importing Required Libraries\n",
        "\n",
        "_We recommend you import all required libraries in one place, as follows:_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YM5mLYDJiJ3"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from itertools import accumulate\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "sns.set_context('notebook')\n",
        "sns.set_style('white')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWLbYOIdJiJ3"
      },
      "source": [
        "## Linear Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9vwlEcJJiJ3"
      },
      "source": [
        "Linear Regression models are typically used to study the relationship between a single dependent variable $Y$ and one or more independent variable $X$.\n",
        "\n",
        "They have an easy-to-interpret mathematical formula that can generate predictions. A simple linear regression model can be used when working with one independent variable, and a multiple regression model can be used when there are more than one independent variables.\n",
        "\n",
        "We can start with a hypothesis that resembles the line, $Y=\\theta_0X+\\theta_1$, where $\\theta_0$ and $\\theta_1$ are the regression coefficients.\n",
        "\n",
        "Now how do we pick the values of ($\\theta_0$) and ($\\theta_1$) so that our model predictions are accurate?\n",
        "\n",
        "We use an optimization method to minimize the loss function so as to reduce the error between model predictions and the ground truth. We start by picking random values of ($\\theta_0$) and ($\\theta_1$), and continue to update values of the coefficients till convergence. If our loss function stops decreasing, we have reached our local minima.\n",
        "\n",
        "In multiple linear regression, we use more than one independent features ($X$) and a single dependent feature ($Y$). If we have $n$ features, our formula is as follows. Instead of considering a vector of ($m$) data entries, we will consider the ($n X m$) matrix of $X$.\n",
        "\n",
        "$Y=\\theta_0+\\theta_1X_1+\\theta_2X_2+\\theta_3X_3+...++\\theta_nX_n$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv1MBxdYJiJ3"
      },
      "source": [
        "### Bike sharing dataset\n",
        "\n",
        "We will be using the bike-sharing dataset from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01). It contains the following features:\n",
        "\n",
        "> - instant: record index\n",
        "> - dteday : date\n",
        "> - season : season (1:winter, 2:spring, 3:summer, 4:fall)\n",
        "> - yr : year (0: 2011, 1:2012)\n",
        "> - mnth : month ( 1 to 12)\n",
        "> - hr : hour (0 to 23)\n",
        "> - holiday : weather day is holiday or not (extracted from [Web Link](https://dchr.dc.gov/page/holiday-schedules?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01))\n",
        "> - weekday : day of the week\n",
        "> - workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n",
        "> - weathersit :(1) Clear, Few clouds, Partly cloudy, Partly cloudy, (2) Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist, (3) Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds, (4) Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
        "> - temp : Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39\n",
        "> - atemp: Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)\n",
        "> hum: Normalized humidity. The values are divided to 100 (max)\n",
        "> windspeed: Normalized wind speed. The values are divided to 67 (max)\n",
        "> casual: count of casual users\n",
        "> registered: count of registered users\n",
        "> cnt: count of total rental bikes including both casual and registered\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwFN3blWJiJ3"
      },
      "source": [
        "Download the dataset, and unzip the `Bike-Sharing-Dataset.zip` folder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yalz-6cJiJ3"
      },
      "outputs": [],
      "source": [
        "import skillsnetwork\n",
        "await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module1/L2/data/Bike-Sharing-Dataset.zip\",overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_ChpdQbJiJ3"
      },
      "source": [
        "We will be using the `day.csv` file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CS5eud8uJiJ3"
      },
      "outputs": [],
      "source": [
        "raw_dataset = pd.read_csv('day.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8qJxurvJiJ3"
      },
      "source": [
        "Let's look at some samples rows from the dataset we loaded:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXbY_IpwJiJ4"
      },
      "outputs": [],
      "source": [
        "raw_dataset.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlU_XFq1JiJ4"
      },
      "source": [
        "### Clean the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyzBR7wyJiJ4"
      },
      "source": [
        "Let us drop the `dteday` column as it is not numerical, and we already have some other date like features like `mnth`, `year` and `weekday`.\n",
        "\n",
        "We will remove `instant` as it is just a row instance identifier.\n",
        "\n",
        "We will also drop `registered` and `casual` as they are used to compute our target variable `cnt` and including them would result in data leakage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5625QX68JiJ4"
      },
      "outputs": [],
      "source": [
        "raw_dataset = raw_dataset.drop(columns=['dteday', 'instant', 'registered', 'casual'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBJ1bXqNJiJ4"
      },
      "source": [
        "### Data exploration: Bivariate analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPtCD5LYJiJ4"
      },
      "source": [
        "We will create scatterplots for a few numerical variables against the target variable `cnt`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5DmCbaqJiJ4"
      },
      "outputs": [],
      "source": [
        "col = ['temp', 'atemp', 'hum', 'windspeed']\n",
        "plt.figure(figsize=[20,12])\n",
        "for i in enumerate(col):\n",
        "    plt.subplot(2,2,i[0]+1)\n",
        "    sns.regplot(data=raw_dataset,x=i[1],y='cnt',line_kws={\"color\":'red'})\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxOWi91eJiJ5"
      },
      "source": [
        "From this, we can infer that there is a positive correlation between `temp` and `atemp` and `cnt`. There is a negative correlation between `windspeed` and `hum` and `cnt`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSl7_7gmJiJ5"
      },
      "source": [
        "Similarly, we can perform a bivariate analysis on the categorical variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHzDSDXXJiJ5"
      },
      "outputs": [],
      "source": [
        "col = ['season', 'yr', 'mnth', 'holiday', 'weekday', 'workingday', 'weathersit']\n",
        "plt.figure(figsize=[20,12])\n",
        "for i in enumerate(col):\n",
        "    plt.subplot(2,4,i[0]+1)\n",
        "    sns.boxplot(data=raw_dataset,x=i[1],y='cnt')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTGa1OwiJiJ5"
      },
      "source": [
        "From this we infer that rental bike demands are the highest during Fall and lowest during Spring. There was an increase in demand in bikes in 2019 as compared to 2018. A higher demand for bikes is observed on non-holidays as compared to holidays. Lastly, when renting bikes, clear weather is preferred over light rain, thunderstorm and snowy days.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBPDH0nRJiJ5"
      },
      "source": [
        "### Data exploration: Multivariate analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ_-7Un-JiJ5"
      },
      "source": [
        "We can perform multivariate analysis by creating a pairplot of all the columns in our dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOR8S_zIJiJ5"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(raw_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_zYRBxTJiJ5"
      },
      "source": [
        "### Data exploration: Heatmap of columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kep7YuVlJiJ5"
      },
      "source": [
        "Using a heatmap we can explore which variables are positively or negatively correlated with one another.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsVCOvi1JiJ6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=[15,8])\n",
        "fig = sns.heatmap(raw_dataset.corr(),cmap='hot_r',\n",
        "            annot=True,linecolor='black',linewidths=0.01,annot_kws={\"fontsize\":12},fmt=\"0.2f\")\n",
        "\n",
        "top, bottom = fig.get_ylim()\n",
        "fig.set_ylim(top+0.1,bottom-0.1)\n",
        "\n",
        "left, right = fig.get_xlim()\n",
        "fig.set_xlim(left-0.1,right+0.1)\n",
        "\n",
        "plt.yticks(fontsize=13,rotation=0)\n",
        "plt.xticks(fontsize=13,rotation=90);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQeGPi4-JiJ6"
      },
      "source": [
        "From the heatmap, we can infer that `temp` and `atemp` are highly correlated with each other and with `cnt`.\n",
        "\n",
        "A positive correlation between `temp`, `atemp`, `yr` and `cnt` exists, while a negative correlation between `windspeed`, `hum`, `weathersit`, `holiday` and `cnt` exists.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFfU1kQJJiJ6"
      },
      "source": [
        "### Split the data into training and test sets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZxd2VtiJiJ6"
      },
      "source": [
        "Now we will split the data set into a training set and a test set. The test set will be used in the final evaluation of your models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EooxFrnzJiJ6"
      },
      "outputs": [],
      "source": [
        "train_dataset = raw_dataset.sample(frac=0.8, random_state=0)\n",
        "test_dataset = raw_dataset.drop(train_dataset.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgeZv2NyJiJ7"
      },
      "source": [
        "### Split features from labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rm8bobsJiJ7"
      },
      "source": [
        "Separate the target value from the features. This label is the value that you will train the model to predict.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_JkvWLeJiJ8"
      },
      "outputs": [],
      "source": [
        "train_features = train_dataset.copy()\n",
        "test_features = test_dataset.copy()\n",
        "\n",
        "train_labels = train_features.pop('cnt')\n",
        "test_labels = test_features.pop('cnt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpBNG4ayJiJ8"
      },
      "source": [
        "### Normalization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja3wGm8IJiJ8"
      },
      "source": [
        "Since the features in this dataset use different scales and ranges, it is generally considered good practice to normalize the features before performing linear regression. This is partly because the features are multiplied by model weights, and the scale of the outputs and gradients are impacted by that of the inputs. Even though it is possible for the model to converge without any feature normalization, the process of normalization makes the training much more stable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4IsoOBbJiJ8"
      },
      "source": [
        "We will use `tf.keras.layers.Normalization` as a clean and simple way to add feature normalization to our model.\n",
        "\n",
        "Let us first create a normalization layer. We will use `axis=-1` as an input argument. As per the [Keras documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Normalization?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01):\n",
        "\n",
        "> The axis or axes that should have a separate mean and variance for each index in the shape. For example, if shape is (None, 5) and axis=1, the layer will track 5 separate mean and variance values for the last axis. If axis is set to None, the layer will normalize all elements in the input by a scalar mean and variance. Defaults to -1, where the last axis of the input is assumed to be a feature dimension and is normalized per index.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HfG_BEHJiJ8"
      },
      "outputs": [],
      "source": [
        "normalizer = tf.keras.layers.Normalization(axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwbD5iMEJiJ8"
      },
      "source": [
        "Now using `normalization.adapt`, we will fit the state of the pre-processing layer to the training data.\n",
        "\n",
        "This will compute the mean and variance of the data and store them as the layer's weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JXtpTqsJiJ9"
      },
      "outputs": [],
      "source": [
        "normalizer.adapt(np.array(train_features))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNiFsy-FJiJ9"
      },
      "source": [
        "Calculate the mean and variance:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEcm-jI3JiJ9"
      },
      "outputs": [],
      "source": [
        "print(normalizer.mean.numpy())\n",
        "print(normalizer.variance.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTndN0CEJiJ9"
      },
      "source": [
        "When the layer is called, it returns the input data, with each feature independently normalized. For example, let us look at the first input row:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOvuLmK2JiJ9"
      },
      "outputs": [],
      "source": [
        "print(np.array(train_features[:1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7wNp-7MJiJ9"
      },
      "source": [
        "Now, let us look at the normalized input:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4L9Mp9xJiJ9"
      },
      "outputs": [],
      "source": [
        "print(normalizer(np.array(train_features[:1])).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ_MilJwJiJ9"
      },
      "source": [
        "### Linear regression with a single input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR0sIgzrJiJ9"
      },
      "source": [
        "We can build a linear regression model using one or several variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji94oXZQJiJ9"
      },
      "source": [
        "Let us begin by building a single-variable linear regression to predict `'cnt'` from `'temp'`. We are using `temp` because it is highly correlated with `cnt` and should be highly predictive.\n",
        "\n",
        "We will use Keras to define a `tf.keras.Sequential` model architecture.\n",
        "\n",
        "Here are the steps we will follow to create our single-variable linear regression model:\n",
        "\n",
        "- Using the `tf.keras.layers.Normalization` preprocessing layer that we created earlier, we will normalize the `'temp'` input features.\n",
        "- Then we will apply a linear transformation to produce 1 output using a linear layer `tf.keras.layers.Dense`.\n",
        "\n",
        "\n",
        "Note that as per the documentation, the input can either be set by the `input_shape` argument, or automatically when the model is run for the first time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tp80Y4yJiJ9"
      },
      "source": [
        "Next, we create an array made of the `temp` features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNZ5kCLDJiJ-"
      },
      "outputs": [],
      "source": [
        "temp = np.array(train_features['temp'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhZvlimKJiJ-"
      },
      "source": [
        "Then we will instantiate the `tf.keras.layers.Normalization` and fit its state to the data like we did before.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eP8VjYnOJiJ-"
      },
      "outputs": [],
      "source": [
        "temp_normalizer = layers.Normalization(input_shape=[1,], axis=None)\n",
        "temp_normalizer.adapt(temp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcuGOax5JiJ-"
      },
      "source": [
        "Build the Keras model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I081prgjJiJ-"
      },
      "outputs": [],
      "source": [
        "temp_model = tf.keras.Sequential([\n",
        "    temp_normalizer,\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "temp_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbfCvZ-gJiJ-"
      },
      "source": [
        "Now, let us configure the training procedure using `model.compile()`. Here, `loss` is used to define the loss function, which computes the quantity that a model should seek to minimize during training. And `optimizer` defines the optimizer that is used to change weights and learning rate during training to minimize losses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VYU9uSFJiJ-"
      },
      "outputs": [],
      "source": [
        "temp_model.compile(\n",
        "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
        "    loss='mean_squared_error')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XZ7rg0sJiJ-"
      },
      "source": [
        "Using `keras.fit`, we train the model for 100 epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-Ujz_TpJiJ_"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "history = temp_model.fit(\n",
        "    train_features['temp'],\n",
        "    train_labels,\n",
        "    # to view the logs, uncomment this:\n",
        "    verbose=False,\n",
        "    epochs=100,\n",
        "    # validation split: 20% of the training data.\n",
        "    validation_split = 0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EXZrsGbJiJ_"
      },
      "source": [
        "We can create a plot of loss on the training and validation datasets over training epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2ayhOKlJiJ_"
      },
      "outputs": [],
      "source": [
        "def plot_loss(history):\n",
        "    plt.plot(history.history['loss'], label='loss')\n",
        "    plt.plot(history.history['val_loss'], label='val_loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCasntqyJiJ_"
      },
      "outputs": [],
      "source": [
        "plot_loss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TE28uiaJiJ_"
      },
      "source": [
        "Next, we will use `model.predict` to predict the `cnt` values in the test dataset, using just the `temp` feature values as input:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oshQ3upnJiKA"
      },
      "outputs": [],
      "source": [
        "y_pred= temp_model.predict(test_features['temp'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvNvq1YcJiKA"
      },
      "source": [
        "We can compute the root mean squared error:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL-Y6VqsJiKA"
      },
      "outputs": [],
      "source": [
        "print(np.sqrt(metrics.mean_squared_error(test_labels, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uct8-P-JiKA"
      },
      "source": [
        "Since this a single-variable linear regression, we can easily visualize the results:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKHvLROHJiKA"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (4,4))\n",
        "plt.plot(test_features['temp'], test_labels,'o',\n",
        "         test_features['temp'],temp_model.predict(test_features['temp']),'g')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF1-YKrjJiKA"
      },
      "source": [
        "### Linear regression with multiple inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jVPN-BjJiKA"
      },
      "source": [
        "We will create a similar setup to predict `cnt` based on multiple inputs. In this case, in our $y = mx+b$ equation, $m$ will be a matrix and $b$ will be a vector.\n",
        "\n",
        "Similar to what we did before, the Sequential model will have `normalizer` as the first layer, and `Dense` as the second layer:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nj0FssJJiKA"
      },
      "outputs": [],
      "source": [
        "linear_model = tf.keras.Sequential([\n",
        "    normalizer,\n",
        "    layers.Dense(units=1)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnUZbD90JiKA"
      },
      "source": [
        "We will compile the model using `complie()` and train the model on 100 epochs using `fit()`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lG95SeXJiKA"
      },
      "outputs": [],
      "source": [
        "linear_model.compile(\n",
        "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
        "    loss='mean_squared_error')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DX7lO9qtJiKA"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "history = linear_model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    epochs=100,\n",
        "    # to view the logs, uncomment this:\n",
        "    verbose=0,\n",
        "    # validation split: 20% of the training data.\n",
        "    validation_split = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vx-Sqc5DJiKA"
      },
      "outputs": [],
      "source": [
        "plot_loss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szc5v4BxJiKA"
      },
      "source": [
        "We can see that with all the inputs in the regression model, it achieves a much lower training and validation error, and RMSE than the `temp_model` that had just one input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3QGaJzrJiKA"
      },
      "outputs": [],
      "source": [
        "y_pred= linear_model.predict(test_features)\n",
        "print(np.sqrt(metrics.mean_squared_error(test_labels, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MUuRfQJJiKA"
      },
      "source": [
        "`get_weights` returns a list consisting of NumPy arrays. The first array gives the weights of the layer and the second array gives the biases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTA1JxqeJiKA"
      },
      "outputs": [],
      "source": [
        "# Print column names for reference\n",
        "print(\"Column names\")\n",
        "print(test_features.columns)\n",
        "\n",
        "# Print weights and biases for each layer\n",
        "for layer in linear_model.layers:\n",
        "    print(\"Layer Name:\", layer.name)\n",
        "    print(\"---\")\n",
        "    print(\"Weights\")\n",
        "    print(\"Shape:\",layer.get_weights()[0].shape,'\\n',layer.get_weights()[0])\n",
        "    print(\"---\")\n",
        "    print(\"Bias\")\n",
        "    print(\"Shape:\",layer.get_weights()[1].shape,'\\n',layer.get_weights()[1],'\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytwXG7TYJiKC"
      },
      "source": [
        "From the model weights, we can see that, similar to what we had seen in our heatmap, `yr`, `temp` and `atemp` have the highest model weights, and hence contribute the most the `cnt` predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urUX4smpJiKC"
      },
      "source": [
        "## Using Neural Networks for Linear Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mL6zMfyJiKC"
      },
      "source": [
        "In this section, we will use a Neural Network architecture in Keras to perform simple linear regression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_1Hz9iDJiKC"
      },
      "source": [
        "Let us begin by reading our univariate dataset. We will use the Iowa Housing dataset where the only feature is SquareFeet, and the target value is SalePrice.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZnFpyaoJiKC"
      },
      "source": [
        "We will reserve the last 500 data points for testing purposes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W93swFfeJiKC"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module1/L2/data/IowaHousingPrices.csv')\n",
        "\n",
        "x_train = df[['SquareFeet']].values[1:-500]\n",
        "y_train = df[['SalePrice']].values[1:-500]\n",
        "x_test = df[['SquareFeet']].values[-500:]\n",
        "y_test = df[['SalePrice']].values[-500:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1PWrVCSJiKC"
      },
      "source": [
        "Now, we'll define the model configuration parameters. Since we're dealing with a univariate dataset, both our input and output sizes are 1. The learning rate is selected arbitrarily, where the goal is to ensure the NN doesn't overfit to the training data, and is easily generalizable to unseen test sets. We will use mean squared error as the loss function, and train the model on 30 iterations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9k9H1SGCJiKD"
      },
      "outputs": [],
      "source": [
        "output_size=1\n",
        "hidden_layer=500\n",
        "input_size=1\n",
        "learning_rate=0.51\n",
        "loss_function='mean_squared_error'\n",
        "epochs=30\n",
        "batch_size=10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5duTnHfWJiKD"
      },
      "source": [
        "We will create a `Sequential` model and add a Dense layer with a relu activation in the hidden layer with ```hidden_layer``` neurons. As we have one output the output layer will have a dimension of  one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dyg2TgFBJiKD"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Dense(hidden_layer,  activation='relu'))\n",
        "\n",
        "model.add(keras.layers.Dense(output_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frvlAXcKJiKD"
      },
      "source": [
        "Next, we will compile and fit the model based on the configuration parameters defined earlier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQw31VTxJiKD"
      },
      "outputs": [],
      "source": [
        "model.compile(keras.optimizers.Adam(lr=learning_rate), loss_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IjVy5pHJiKD"
      },
      "outputs": [],
      "source": [
        "model.fit(x_train, y_train, epochs = epochs, batch_size = batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb7AAifaJiKE"
      },
      "source": [
        "Let us predict the sales prices on the full range of the test set and plot the result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iGLREhaJiKE"
      },
      "outputs": [],
      "source": [
        "x=np.arange(x_test.min(),x_test.max(),10).reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9Jz9GUrJiKE"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUZk3ebaJiKE"
      },
      "outputs": [],
      "source": [
        "plt.plot(x,y_pred,label=\"prediction \")\n",
        "plt.plot(x_test,y_test,'ro',label=\"test samples\")\n",
        "plt.xlabel('Input')\n",
        "plt.ylabel('Predicted Output')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE3IunshJiKE"
      },
      "source": [
        "The model performs relatively well on the test set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QldkRXbJJiKE"
      },
      "source": [
        "# Exercises\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOGJxxGGJiKE"
      },
      "source": [
        "In this section, you will train a linear regression model in Keras. We will be using the classic Auto MPG dataset, and predict the fiel efficiency of automobiles.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UavELVFmJiKE"
      },
      "source": [
        "### Exercise 1 - Loading a dataset (Auto MPG)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8E1Alc_JiKE"
      },
      "source": [
        "The dataset is again found in the UCI Machine Learning Repository. You can download it from [here](http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjYtJU7-JiKE"
      },
      "outputs": [],
      "source": [
        "# Write your solution here:\n",
        "\n",
        "url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module1/L2/data/auto-mpg.data'\n",
        "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
        "                'Acceleration', 'Model Year', 'Origin']\n",
        "\n",
        "raw_dataset = # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2Ab_8soJiKF"
      },
      "source": [
        "<details>\n",
        "    <summary>Click here for Solution</summary>\n",
        "\n",
        "```python\n",
        "raw_dataset = pd.read_csv(url, names=column_names,\n",
        "                          na_values='?', comment='\\t',\n",
        "                          sep=' ', skipinitialspace=True)\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0UQhy0AJiKF"
      },
      "source": [
        "### Exercise 2 - Clean the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CsqCuGBJiKF"
      },
      "source": [
        "Drop all the rows that have NA values, and remove the `origin` column as it is categorical.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVe_anQaJiKF"
      },
      "outputs": [],
      "source": [
        "# Write your solution here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49OMHBmVJiKF"
      },
      "source": [
        "<details>\n",
        "    <summary>Click here for Solution</summary>\n",
        "\n",
        "```python\n",
        "dataset = raw_dataset.dropna()\n",
        "dataset = dataset.drop(columns=['Origin'])\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Q3En4w0JiKF"
      },
      "source": [
        "### Exercise 3 - Split the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-RIGxTBJiKF"
      },
      "source": [
        "Split the data into training and test, and split the target from features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjlnejhYJiKF"
      },
      "outputs": [],
      "source": [
        "# Write your solution here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVAN2hAAJiKF"
      },
      "source": [
        "<details>\n",
        "    <summary>Click here for Solution</summary>\n",
        "\n",
        "```python\n",
        "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
        "test_dataset = dataset.drop(train_dataset.index)\n",
        "\n",
        "train_features = train_dataset.copy()\n",
        "test_features = test_dataset.copy()\n",
        "\n",
        "train_labels = train_features.pop('MPG')\n",
        "test_labels = test_features.pop('MPG')\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8wqJNWhJiKF"
      },
      "source": [
        "### Exercise 4 - Create a normalization layer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfLEGy0vJiKG"
      },
      "source": [
        "Next, use `tf.keras.layers.Normalization` to add feature normalization to the model. Using `normalization.adapt`, fit the state of the pre-processing layer to the training data. Calculate and print the mean and variance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7S3JsydJiKG"
      },
      "outputs": [],
      "source": [
        "# Write your solution here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJfOeLpaJiKG"
      },
      "source": [
        "<details>\n",
        "    <summary>Click here for Solution</summary>\n",
        "\n",
        "```python\n",
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "normalizer.adapt(np.array(train_features))\n",
        "print(normalizer.mean.numpy())\n",
        "print(normalizer.variance.numpy())\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnglXwnPJiKG"
      },
      "source": [
        "### Exercise 5 - Linear regression with multiple inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdOez-IZJiKG"
      },
      "source": [
        "Create a linear regression model to predict `MPG` based on all inputs in the dataset. In this case, in our $y = mx+b$ equation, $m$ will be a matrix and $b$ will be a vector.\n",
        "\n",
        "Similar to what we did before, the Sequential model will have `normalizer` as the first layer, and `Dense` as the second layer. Compile and train the model, and plot the training history.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UY02nv1dJiKG"
      },
      "outputs": [],
      "source": [
        "# Write your solution here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGbwgb1iJiKG"
      },
      "source": [
        "<details>\n",
        "    <summary>Click here for Solution</summary>\n",
        "\n",
        "```python\n",
        "linear_model = tf.keras.Sequential([\n",
        "    normalizer,\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "linear_model.compile(\n",
        "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
        "    loss='mean_squared_error')\n",
        "\n",
        "history = linear_model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    epochs=100,\n",
        "    # to view the logs, uncomment this:\n",
        "    verbose=0,\n",
        "    # validation split: 20% of the training data.\n",
        "    validation_split = 0.2)\n",
        "\n",
        "plot_loss(history)\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7mg2_BeJiKG"
      },
      "source": [
        "Predict the output values of the test set, and compute the RMSE.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b64q_gJJiKG"
      },
      "outputs": [],
      "source": [
        "# Write your solution here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dPMMDB2JiKG"
      },
      "source": [
        "<details>\n",
        "    <summary>Click here for Solution</summary>\n",
        "\n",
        "```python\n",
        "\n",
        "y_pred= linear_model.predict(test_features)\n",
        "print(np.sqrt(metrics.mean_squared_error(test_labels, y_pred)))\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LsUdJxxJiKG"
      },
      "source": [
        "## Authors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uLtkNGVJiKG"
      },
      "source": [
        "[Kopal Garg](https://www.linkedin.com/in/gargkopal/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01) is a Data Scientist Intern at IBM, and a Masters student in Computer Science at the University of Toronto.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_2SwLTcJiKG"
      },
      "source": [
        "## Change Log\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woLlqCi_JiKG"
      },
      "source": [
        "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
        "|-|-|-|-|\n",
        "|2022-07-06|0.1|Kopal|Create Lab||2022-08-09|0.2|Steve H.|QA pass|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI1xcLxmJiKH"
      },
      "source": [
        "Copyright  2022 IBM Corporation. All rights reserved.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "conda-env-python-py"
    },
    "language_info": {
      "name": ""
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}